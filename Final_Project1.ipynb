{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import randrange\n",
    "import random\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "    '''\n",
    "    Load data from .csv file\n",
    "    Arguments: \n",
    "          filename: File name [string]\n",
    "    Outputs: \n",
    "        dataset: Loaded dataset [numpy array]\n",
    "    '''\n",
    "    dataset=np.genfromtxt(filename, delimiter=',', dtype=\"|U5\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rescale dataset columns to the range 0-1\n",
    "def normalize_dataset(dataset):\n",
    "    '''\n",
    "    Normalizing dataset in range of (0,1)\n",
    "    Arguments: \n",
    "         dataset: The input data [numpy]\n",
    "    Outputs: \n",
    "         dataset: Normalized dataset with all elements of each data point in range of (0,1) [numpy]\n",
    "    '''\n",
    "    dataset = (dataset-dataset.min(axis=1,keepdims=True))/(dataset.max(axis=1,keepdims=True)-dataset.min(axis=1,keepdims=True)) \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize dataset columns\n",
    "def standardize_dataset(dataset):\n",
    "    '''\n",
    "    Standardizing dataset \n",
    "    Arguments: \n",
    "         dataset: The input data [numpy]\n",
    "    Outputs: \n",
    "         dataset: Standardize dataset [numpy]\n",
    "    '''\n",
    "    mean_val = np.mean(dataset, axis=0)\n",
    "    dataset = dataset - mean_val\n",
    "    std_val = np.std(dataset, axis=0)\n",
    "    dataset = dataset/std_val\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "              \n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "    '''\n",
    "    Split data randomly into k folds for cross validation\n",
    "    Arguments: \n",
    "         dataset: The input data [numpy],\n",
    "         n_folds: Number of required folds to be split into [int]\n",
    "    Outputs: \n",
    "         train_index: Indices of the random section chosen as the train data in each step of learning [numpy array], \n",
    "         test_index: Indices of the random section chosen as the test data in each step of learning [numpy array]\n",
    "    '''\n",
    "    \n",
    "    # find size of each fold = find size of test slice\n",
    "    test_size = dataset.shape[0] / n_folds\n",
    "    test_size = np.rint(test_size)        \n",
    "    test_size = test_size.astype(int)\n",
    "        \n",
    "    low_range = 0\n",
    "    high_range = dataset.shape[0]\n",
    "    \n",
    "    # randomly devide data into test and train slices of specified sizes\n",
    "    test_index = random.sample(range(low_range, high_range), test_size)\n",
    "    all_indices = np.arange(low_range,high_range)\n",
    "    train_index = np.delete(all_indices, test_index)\n",
    "   \n",
    "    \n",
    "    return train_index, test_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "    '''\n",
    "    Calculate the accuracy score according to the predicted and actual labels\n",
    "    Arguments: \n",
    "         actual: Real labels of train data [int], \n",
    "         predicted: The implemented model's predicted labels [int]\n",
    "    Outputs: \n",
    "         accuracy: Accuracy score in percentage [float] \n",
    "    '''\n",
    "    tolerance = 1e-10\n",
    "    accuracy = (np.abs(predicted - actual[:, np.newaxis]) < tolerance ).mean()*100\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find coefficients and apply on test_slice of training data\n",
    "def find_coeffs(y, tx, y_test, tx_test, algorithm, *args):\n",
    "    '''\n",
    "    Finds W coefficients using specified algorithm and gradient descent\n",
    "    Arguments: \n",
    "         y: Corresponding labels of the train data [numpy array],\n",
    "         tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "         y_test: Corresponding labels of the test data [numpy array],\n",
    "         tx_test: The random section chosen as the test data in each step of learning [numpy array],        \n",
    "         algorithm: Chosen algorithm for classification; It could be 'least_squares'/ 'least_squares_GD'/\n",
    "         'least_squares_SGD'/'logistic_regression'/'reg_logistic_regression'/'ridge_regression' [name of function]\n",
    "    Outputs: \n",
    "         coef: Classification coefficients [float],\n",
    "         loss: Loss function calculated for the method [float], \n",
    "         accuracy: Mean accuracy score of the used algorithm on test slice [float]\n",
    "    '''\n",
    "        \n",
    "    # apply specified classification method on train slice and define W coefficients\n",
    "    if (algorithm == 'least_squares_GD' ):\n",
    "        func = least_squares_GD\n",
    "        coef, loss = func(y, tx, initial_w, max_iters, gamma) \n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "    \n",
    "    if (algorithm == 'least_squares_SGD' ):\n",
    "        func = least_squares_SGD\n",
    "        coef, loss = func(y, tx, initial_w, max_iters, gamma)\n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "    \n",
    "    if (algorithm == 'least_squares' ):\n",
    "        func = least_squares\n",
    "        coef, loss = func(y, tx)  \n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "\n",
    "    \n",
    "    if (algorithm == 'ridge_regression' ):\n",
    "        func = ridge_regression\n",
    "        coef, loss = func(y, tx, lambda_) \n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "        y_hat = y_hat[:, np.newaxis]\n",
    "\n",
    "    \n",
    "    if (algorithm == 'logistic_regression' ):\n",
    "        func = logistic_regression\n",
    "        coef, loss = func(y, tx, initial_w, max_iters, gamma)  \n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "    \n",
    "    if (algorithm == 'reg_logistic_regression' ):\n",
    "        func = reg_logistic_regression\n",
    "        coef, loss = func(y, tx, lambda_, initial_w, max_iters, gamma) \n",
    "        \n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        y_hat = predict_sigmoid(tx_test, coef)\n",
    "    \n",
    "        \n",
    "   \n",
    "    \n",
    "    # cast predicted labels to binary class identifiers with 0.6 treshhold\n",
    "    y_hat[y_hat>=0.6] = 1\n",
    "    y_hat[y_hat<0.6] = 0\n",
    "    \n",
    "    \n",
    "    # compute accuracy measure\n",
    "    accuracy = accuracy_metric(y_test, y_hat)\n",
    "    \n",
    "    #predictions = np.append(predictions,yhat)\n",
    "    return (coef, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(y, tx):\n",
    "    '''\n",
    "    Finds W coefficients using least squares method\n",
    "    Arguments: \n",
    "        tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "        y: Corresponding labels of the chosen data [numpy array],\n",
    "    Outputs: \n",
    "        w: Classification coefficients [float],\n",
    "        loss: Loss function calculated for the method [float]       \n",
    "    '''\n",
    "    \n",
    "    # prepare data\n",
    "    a = tx.T.dot(tx)\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    # compute regression coefficients\n",
    "    w = np.linalg.solve(a, b)\n",
    "    \n",
    "    # calculate predicted labels \n",
    "    yhat = np.matmul(tx,w)\n",
    "    \n",
    "    # calculate error\n",
    "    error = (y[:, np.newaxis] - yhat[:, np.newaxis])\n",
    "    \n",
    "    # compute loss function\n",
    "    loss = ((np.matmul(error.T,error))/(yhat.shape[0]))\n",
    "    \n",
    "    return w[:, np.newaxis], loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate least square coefficients using gradient descent\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Finds W coefficients using least square loss function with gradient descent\n",
    "    Arguments: \n",
    "         y: The labels of randomly chosen train data [numpy array],\n",
    "         tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "         initial_w: Initial value for the weight vector [numpy array],\n",
    "         max_iters: Maximum number of steps to run [int],\n",
    "         gamma: Learning rate of gradient descent method [float]\n",
    "         \n",
    "    Outputs: \n",
    "         w: Classification coefficients [float],\n",
    "         loss: Loss function calculated for the method [float]\n",
    "    '''  \n",
    "    # Initilizing wights\n",
    "    w = initial_w\n",
    "    \n",
    "    # apply gradient descent on least square loss function until meeting the max number of iterations\n",
    "    for epoch in range(max_iters):\n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        yhat = predict_sigmoid(tx, w) # This function should be changed\n",
    "        # compute prediction error\n",
    "        error = (y[:, np.newaxis] - yhat)\n",
    "\n",
    "        # gradient descent\n",
    "        gradient = np.matmul(error.T,tx)/(tx.shape[0])\n",
    "        gradient = gamma * gradient\n",
    "\n",
    "        # update coefficients\n",
    "        w = w + gradient.T\n",
    "     \n",
    "    loss = ((np.matmul(error.T,error))/(yhat.shape[0]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate least square coefficients using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Finds W coefficients using least square loss function with gradient descent\n",
    "    Arguments: \n",
    "         y: The labels of randomly chosen train data [numpy array],\n",
    "         tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "         initial_w: Initial value for the weight vector [numpy array],\n",
    "         max_iters: Maximum number of steps to run [int],\n",
    "         gamma: Learning rate of gradient descent method [float]\n",
    "        \n",
    "    Outputs: \n",
    "         w: Classification coefficients [float],\n",
    "         loss: Loss function calculated for the method [float]\n",
    "    '''  \n",
    "    # Initilizing wights and batch size\n",
    "    w = initial_w\n",
    "    B_size = 1\n",
    "    \n",
    "    # apply gradient descent on least square loss function until meeting the max number of iterations\n",
    "    for epoch in range(max_iters):\n",
    "        # Selecting one \n",
    "        B_idx = random.sample(range(0, tx.shape[0]), B_size)\n",
    "        # calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "        yhat = predict_sigmoid(tx_train[B_idx,:], w) # This function should be changed\n",
    "        # compute prediction error\n",
    "        error = (y_train[B_idx, np.newaxis] - yhat)\n",
    "\n",
    "        # gradient descent\n",
    "        gradient = np.matmul(error.T,tx_train[B_idx,:])/(tx_train[B_idx,:].shape[0])\n",
    "        gradient = gamma * gradient\n",
    "\n",
    "        # update coefficients\n",
    "        w = w + gradient.T\n",
    "     \n",
    "    loss = ((np.matmul(error.T,error))/(yhat.shape[0]))\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate logistic regression coefficients using gradient descent\n",
    "def logistic_regression (y, tx, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Finds W coefficients using least square loss function with gradient descent\n",
    "    Arguments: \n",
    "         y: The labels of randomly chosen train data [numpy array],\n",
    "         tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "         initial_w: Initial value for the weight vector [numpy array],\n",
    "         max_iters: Maximum number of steps to run [int],\n",
    "         gamma: Learning rate of gradient descent method) [float]\n",
    "        \n",
    "    Outputs: \n",
    "        w: Classification coefficients [float],\n",
    "        loss: Loss function calculated for the method [float]\n",
    "    '''   \n",
    "    # initialize coefficients with zero\n",
    "    w = initial_w\n",
    "    \n",
    "    error =[]\n",
    "    gradient =[]\n",
    "    \n",
    "    # apply gradient descent on least square loss function until meeting the max number of iterations\n",
    "    for epoch in range(max_iters):\n",
    "   \n",
    "            #  calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "            yhat = predict_sigmoid(tx, w)\n",
    "            \n",
    "            # compute prediction error\n",
    "            error = (y[:, np.newaxis] - yhat)\n",
    "            \n",
    "            # gradient descent\n",
    "            multiplied = np.matmul(tx,w)\n",
    "            sigma = 1/ (1 + np.exp(-multiplied))\n",
    "            gradient = np.matmul(tx.T, sigma-yhat) \n",
    "            gradient = gamma * gradient\n",
    "            \n",
    "            # update coefficients\n",
    "            w = w - gradient\n",
    "        \n",
    "    loss = (np.matmul(-y.T, np.log(yhat)) - np.matmul((1 -y.T), np.log(1 - yhat)))/(yhat.shape[0])     \n",
    "    return  w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate regularized logistic regression coefficients using gradient descent\n",
    "def reg_logistic_regression (y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    '''\n",
    "    Finds W coefficients using least square loss function with gradient descent\n",
    "    Arguments:      \n",
    "         y: The labels of randomly chosen train data [numpy array],\n",
    "         tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "         lambda_ : Regularization parameter [float],\n",
    "         initial_w: Initial value for the weight vector [numpy array],\n",
    "         max_iters: Maximum number of steps to run [int],\n",
    "         gamma: Learning rate of gradient descent method) [float]\n",
    "        \n",
    "    Outputs: \n",
    "        w: Classification coefficients [float],\n",
    "        loss: Loss function calculated for the method [float]\n",
    "    '''   \n",
    "    # initialize coefficients with zero\n",
    "    w = initial_w\n",
    "    \n",
    "    error =[]\n",
    "    gradient =[]\n",
    "    \n",
    "    # apply gradient descent on least square loss function until meeting the max number of iterations\n",
    "    for epoch in range(max_iters):\n",
    "   \n",
    "            #  calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "            yhat = predict_sigmoid(tx, w)\n",
    "            \n",
    "            # compute prediction error\n",
    "            error = (y[:, np.newaxis] - yhat)\n",
    "            \n",
    "            # gradient descent\n",
    "            multiplied = np.matmul(tx,w)\n",
    "            sigma = 1/ (1 + np.exp(-multiplied))\n",
    "            gradient = np.matmul(tx.T, sigma-yhat) + lambda_ * w\n",
    "            gradient = gamma * gradient\n",
    "            \n",
    "            # update coefficients\n",
    "            w = w - gradient\n",
    "        \n",
    "    loss = (np.matmul(-y.T, np.log(yhat)) - np.matmul((1 -y.T), np.log(1 - yhat)))/(yhat.shape[0])  + lambda_/2 *(np.linalg.norm(w))   \n",
    "    return  w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate ridge regression coefficients using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    '''\n",
    "    Finds W coefficients using ridge regression loss function\n",
    "    Arguments: \n",
    "        y: Corresponding labels of the chosen data [numpy array],\n",
    "        tx: The random section chosen as the train data in each step of learning [numpy array], \n",
    "        lambda_ : Regularization parameter [float]\n",
    "    Outputs: \n",
    "        w: Classification coefficients [float],\n",
    "        loss: Loss function calculated for the method [float]       \n",
    "    '''\n",
    "    \n",
    "    # apply regularization parameter on data\n",
    "    aI = lambda_ * np.identity(tx.shape[1])\n",
    "    a = tx.T.dot(tx) + aI\n",
    "    b = tx.T.dot(y)\n",
    "    \n",
    "    # compute regression coefficients\n",
    "    w = np.linalg.solve(a, b)\n",
    "    \n",
    "    # calculate predicted labels \n",
    "    yhat = np.matmul(tx,w)\n",
    "    \n",
    "    # calculate error\n",
    "    error = (y[:, np.newaxis] - yhat[:, np.newaxis])\n",
    "    \n",
    "    # compute loss function\n",
    "    loss = ((np.matmul(error.T,error))/(yhat.shape[0]))\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "def predict_sigmoid(data, coefficients):\n",
    "    '''\n",
    "    Calculates yhat and applies sigmoid function on it\n",
    "    Arguments: \n",
    "        data: Dataset to predict its labels [numpy array], \n",
    "        coefficients: Classification coefficients [float]\n",
    "    Outputs: \n",
    "        yhat: Predicted labels [float]\n",
    "    '''    \n",
    "    yhat = np.matmul(data,coefficients)\n",
    "    return 1.0 / (1.0 + np.exp(-yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bound outliers \n",
    "def data_bounded(X_d, times_SE=3.0):\n",
    "    '''\n",
    "    Arguments:\n",
    "        X_d: Standardized data matrix to be bounded by 3 times of SE [Numpy array]\n",
    "    Output:\n",
    "        X_bounded: Bounded data matrix [Numpy Array]\n",
    "    \n",
    "    '''\n",
    "    X_b = X_d\n",
    "    for i in range(X_b.shape[1]):\n",
    "        X_i = X_b[:,i]\n",
    "        X_i[(X_i <-times_SE) | (X_i>times_SE)] = np.mean(X_i[(X_i >=-times_SE) & (X_i<=times_SE)])\n",
    "        X_b[:,i] = X_i\n",
    "    \n",
    "    return X_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean undefined, unmeasured, or invalid data\n",
    "def data_cleaning(X):\n",
    "    '''\n",
    "    Arguments:\n",
    "        X: Data matrix [Numpy array]\n",
    "    Outputs:\n",
    "        X_c: Cleaned data [Numpy array]\n",
    "    '''\n",
    "    X_c = np.copy(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        X_i = X[:,i]\n",
    "        X_i[X_i == -999] = np.mean(X_i[X_i != -999])\n",
    "        X_c[:,i] = X_i\n",
    "        \n",
    "    return X_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "seed(1)\n",
    "\n",
    "# load and prepare data\n",
    "train_data = load_csv('train.csv')\n",
    "\n",
    "# omit first row(header) and first column(data ids)\n",
    "train_data = train_data[1:,1:]\n",
    "\n",
    "# replace labels 's' and 'b' with '1' and '0'\n",
    "train_data[:,0][train_data[:,0]=='s']='1'\n",
    "train_data[:,0][train_data[:,0]=='b']='0'\n",
    "\n",
    "# cast data type to float\n",
    "train_data = train_data.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and clear data\n",
    "\n",
    "# separate labels\n",
    "y = np.copy(train_data[:,0])\n",
    "\n",
    "X = np.copy(train_data[:,1:])\n",
    "\n",
    "# clean dataset\n",
    "X = data_cleaning(X)\n",
    "\n",
    "# standardize dataset\n",
    "X = standardize_dataset(X)\n",
    "\n",
    "# bound dataset\n",
    "tx = data_bounded(X, times_SE = 3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy Score: 74.17399999999999\n",
      "Loss Value: 0.18612283382506395\n"
     ]
    }
   ],
   "source": [
    "# Evaluate an algorithm using a cross validation split\n",
    "\n",
    "\n",
    "# number of folds for cross validation\n",
    "n_folds = 5\n",
    "\n",
    "# learning rate for gradient descent (step size)\n",
    "gamma = 0.8\n",
    "\n",
    "# maximum number of iterations to perform\n",
    "max_iters = 100\n",
    "\n",
    "# regularization parameter\n",
    "lambda_ = 0.1\n",
    "\n",
    "scores = []\n",
    "num_folds = [n_folds]\n",
    "    \n",
    "# Initilize initial_w\n",
    "initial_w = np.zeros((tx.shape[1], 1))\n",
    " \n",
    "\n",
    "for fold in num_folds:\n",
    "    # apply k-fold cross validation on train data and seperate it into two slices: train_slice, test_slice\n",
    "    train_idx, test_idx  = cross_validation_split(tx, n_folds)\n",
    "    \n",
    "    tx_train, tx_test = tx[train_idx,:], tx[test_idx,:]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # find W coefficients using the specified algorithm and calculate accuracy score using predicted data labels\n",
    "    \n",
    "    # Least Square GD\n",
    "    coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test, 'least_squares_GD', initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Least Square SGD\n",
    "    #coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test, 'least_squares_SGD', initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Least Square Normal Equation\n",
    "    #coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test,'least_squares')\n",
    "    \n",
    "    # Ridge Regression\n",
    "    #coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test, 'ridge_regression', lambda_)\n",
    "    \n",
    "    #Logistic Regression\n",
    "    #coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test, 'logistic_regression', initial_w, max_iters, gamma)\n",
    "    \n",
    "    # Regularized Logistic Regression\n",
    "    #coefficients, loss, accuracy = find_coeffs(y_train, tx_train, y_test, tx_test, 'reg_logistic_regression', lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    scores = np.append(scores, accuracy)\n",
    "        \n",
    "        \n",
    "# report results\n",
    "print('Mean Accuracy Score: %s' % np.mean(scores))\n",
    "#print('The Last Weight Vector Of The Method:', coefficients)\n",
    "print('Loss Value: %s' % loss[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run On Test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run algorithm on train data\n",
    "\n",
    "# load and prepare test data\n",
    "test_data = load_csv('test.csv')\n",
    "\n",
    "# omit first row(header) and the first two columns(data ids & unknown labels '?')\n",
    "test_data = test_data[1:,2:]\n",
    "\n",
    "# cast data type to float\n",
    "test_data = test_data.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize and clear data\n",
    "\n",
    "# clear dataset\n",
    "test_data = data_cleaning(test_data)\n",
    "\n",
    "# standardize dataset\n",
    "test_data = standardize_dataset(test_data)\n",
    "\n",
    "# bound dataset\n",
    "test_data = data_bounded(test_data, times_SE = 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict tes labels\n",
    "\n",
    "predictionsTest = []\n",
    "\n",
    "# calculate yhat predictions and apply sigmoid function on predicted labels\n",
    "yhat_test = predict_sigmoid(test_data, coefficients)\n",
    "\n",
    "# cast predicted labels to binary class identifiers with 0.6 treshhold\n",
    "yhat_test[yhat_test>=0.6] = 1\n",
    "yhat_test[yhat_test<0.6] = 0\n",
    "\n",
    "# predicted labels\n",
    "predictionsTest = np.append(predictionsTest,yhat_test)\n",
    "\n",
    "\n",
    "# set the labels to +/- 1\n",
    "predictionsTest[predictionsTest == 0]= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Output .csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output .csv file\n",
    "import csv\n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    Creates an output file in csv format for submission to kaggle\n",
    "    Arguments: \n",
    "          ids: event ids associated with each prediction\n",
    "          y_pred: predicted class labels \n",
    "          name: string name of .csv output file to be created\n",
    "               \n",
    "    Outputs: \n",
    "         testLabels: File containing predicted test lables [.csv]\n",
    "    \"\"\"\n",
    "    with open(name, 'w') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})\n",
    "\n",
    "            \n",
    "ids = np.arange(350000, 918238)\n",
    "create_csv_submission(ids, predictionsTest, 'testLabels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
